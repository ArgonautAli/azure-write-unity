{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install pandas\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish connection with azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "storage_account = \"aditestacc\"\n",
    "account_key = \"secret_key_here\"\n",
    "container_name = \"testcontainer\"\n",
    "folder = \"dir\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyDockerSparkApp\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-azure:3.3.0,com.microsoft.azure:azure-storage:8.6.6\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"fs.azure.impl\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\")\n",
    "print(\"PySpark version:\", pyspark.__version__)\n",
    "hadoop_version = spark.sparkContext._jvm.org.apache.hadoop.util.VersionInfo.getVersion()\n",
    "print(\"Hadoop version:\", hadoop_version)  \n",
    "blob_container_url = f\"wasbs://{container_name}@{storage_account}.blob.core.windows.net/{folder}\"\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account}.blob.core.windows.net\", account_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define writer function to add to unity catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_df_to_unity_catalog(pandas_df, blob_container_url, database_name, table_name, file_format='parquet'):\n",
    "    \"\"\"\n",
    "    Write a Pandas DataFrame to Azure Blob Storage and create an external table in Unity Catalog.\n",
    "    \"\"\"\n",
    "\n",
    "    spark_df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "    storage_path = f\"{blob_container_url}/{table_name}\"\n",
    "    \n",
    "    spark_df.write.mode(\"overwrite\").format(file_format).save(storage_path)\n",
    "    \n",
    "    create_table_sql = f\"\"\"\n",
    "    CREATE TABLE {database_name}.{table_name}\n",
    "    USING {file_format}\n",
    "    LOCATION '{storage_path}'\n",
    "    \"\"\"\n",
    "    spark.sql(create_table_sql)\n",
    "    \n",
    "    print(f\"External table {database_name}.{table_name} created successfully with data at {storage_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usecase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "  \"calories\": [420, 380, 390],\n",
    "  \"duration\": [50, 40, 45]\n",
    "}\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "db_name = \"adi_test_catalog\"\n",
    "tb_name = \"untiy_tb\"\n",
    "\n",
    "write_df_to_unity_catalog(df, blob_container_url, db_name, tb_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
